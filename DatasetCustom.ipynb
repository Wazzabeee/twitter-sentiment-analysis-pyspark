{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Modèle Logistic Regression avec dataset personnel\n",
    "## Comprend un pré-traitement du dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef81321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer, CountVectorizer, NGram, VectorAssembler, ChiSqSelector\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Variables de contexte et chargement du dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393be6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark1 = SparkSession.builder\\\n",
    "            .master(\"local[16]\")\\\n",
    "            .appName(\"TOT\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e9dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../resources/training_noemoticon.csv\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"target\", IntegerType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"query\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"tweet\", StringType(), True)])\n",
    "\n",
    "df = spark1.read.csv(path,\n",
    "                     inferSchema=True,\n",
    "                     header=False,\n",
    "                     schema=schema)\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classe héritant de Pyspark Transformer afin d'être utilisable en Pipeline ou avec un nomdeclasse.transform(dataset)\n",
    "\n",
    "__init__ :\n",
    "initialise les noms de colonnes, le regex permettant de filtrer les URLs et les mots de ponctuations non désirés\n",
    "\n",
    "\n",
    " _transform:\n",
    " applique la transformation sur un dataset donné\n",
    "\n",
    " Attention : le processus n'est pas bien optimisé et mettra plus de temps à se réaliser\n",
    " mais cela améliorera l'accuracy\n",
    "\"\"\"\n",
    "class WordFormatter(Transformer):\n",
    "    def __init__(self, *, inputCol, outputCol):\n",
    "\n",
    "        super(WordFormatter, self).__init__()\n",
    "        self.regpat = re.compile(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b')\n",
    "        self.ponc = [',', '.', '?', '-']\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "\n",
    "    def stopw(self, wt):\n",
    "\n",
    "        filtered_sentence = []\n",
    "\n",
    "        # pour tout les mots\n",
    "        for w in wt[\"filtered\"]:\n",
    "            wrd = w.strip().lower()\n",
    "\n",
    "            # on vérifie si la ponctuation non désirée est présente, on l'enlève si oui\n",
    "            for char in self.ponc:\n",
    "                wrd = wrd.replace(char, '')\n",
    "\n",
    "            # on ne garde pas les mots avec un URL, mot vide, mot ' ' ou des mots avec '@'\n",
    "            if not re.fullmatch(self.regpat, wrd) and wrd != ' ' and wrd != '' and '@' not in wrd:\n",
    "                filtered_sentence.append(wrd)\n",
    "\n",
    "        # renvoie la ligne telle qu'elle était mais avec notre liste en plus\n",
    "        return wt[0], wt[1], wt[2], wt[3], wt[4], wt[5], wt[6], filtered_sentence\n",
    "\n",
    "    def _transform(self, dtf: DataFrame) -> DataFrame:\n",
    "\n",
    "        # applique le stop words de PySpark\n",
    "        rem = StopWordsRemover(inputCol=self.inputCol, outputCol=\"filtered\")\n",
    "        ndtf = rem.transform(dtf)\n",
    "\n",
    "        # applique notre fonction en plus pour mieux filtrer\n",
    "        rdd = ndtf.rdd.map(lambda x: self.stopw(x))\n",
    "        ndtf = rdd.toDF()\n",
    "\n",
    "        # remets les noms de colonne de départ\n",
    "        official_col = ['target', 'id', 'date', 'query', 'author', 'tweet', self.inputCol, self.outputCol]\n",
    "\n",
    "        for old, new in zip(ndtf.columns, official_col):\n",
    "            ndtf = ndtf.withColumnRenamed(old, new)\n",
    "\n",
    "        return ndtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train_set, test_set) = df.randomSplit([0.80, 0.20], seed = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be6ad42",
   "metadata": {},
   "source": [
    "## HashingTF - IDF (paramètres par défaut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3834df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"tk\")\n",
    "wordform = WordFormatter(inputCol=\"tk\", outputCol=\"words\")\n",
    "hashtf = HashingTF(inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\")\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "\n",
    "lr = LogisticRegression()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, wordform, hashtf, idf, label_stringIdx, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0536aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "\n",
    "predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40891d71",
   "metadata": {},
   "source": [
    "## HashingTF - IDF (paramètres customisés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b73ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"tk\")\n",
    "wordform = WordFormatter(inputCol=\"tk\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "\n",
    "lr = LogisticRegression()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, wordform, hashtf, idf, label_stringIdx, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db2db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "\n",
    "predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8356be2",
   "metadata": {},
   "source": [
    "## CountVectorizer - IDF (paramètres par défaut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab87e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"tk\")\n",
    "wordform = WordFormatter(inputCol=\"tk\", outputCol=\"words\")\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\")\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "\n",
    "lr = LogisticRegression()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, wordform, cv, idf, label_stringIdx, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d9a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "\n",
    "predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca8ad0",
   "metadata": {},
   "source": [
    "## CountVectorizer - IDF (paramètres customisés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7489729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"tk\")\n",
    "wordform = WordFormatter(inputCol=\"tk\", outputCol=\"words\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "\n",
    "lr = LogisticRegression()\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, wordform, cv, idf, label_stringIdx, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipelineFit = pipeline.fit(train_set)\n",
    "\n",
    "predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6227e0f9",
   "metadata": {},
   "source": [
    "# CountVectorizer + NGram + ChisQSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee5f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigrams(inputCol=[\"tweet\",\"target\"], n=3):\n",
    "    \n",
    "    tokenizer = [Tokenizer(inputCol=\"tweet\", outputCol=\"tk\")]\n",
    "    wordform = [WordFormatter(inputCol=\"tk\", outputCol=\"words\")]\n",
    "\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=2**14,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    \n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"rawFeatures\"\n",
    "    )]\n",
    "    \n",
    "    label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
    "    \n",
    "    selector = [ChiSqSelector(numTopFeatures=2**14,featuresCol='rawFeatures', outputCol=\"features\")]\n",
    "    \n",
    "    lr = [LogisticRegression()]\n",
    "    \n",
    "    return Pipeline(stages=tokenizer + wordform + ngrams + cv + idf + assembler + label_stringIdx + selector + lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311a979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipelineFit = build_trigrams().fit(train_set)\n",
    "\n",
    "predictions = pipelineFit.transform(test_set)\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
